import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
img_height, img_width = 224, 224
batch_size = 32
epochs = 50
initial_learning_rate = 1e-4

# ‚úÖ Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÑ‡∏°‡πà‡∏°‡∏µ Data Augmentation)
train_datagen = ImageDataGenerator(rescale=1./255)

# ‚úÖ ‡πÅ‡∏ö‡πà‡∏á `DataTest` ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏™‡πà‡∏ß‡∏ô (50% Validation, 50% Test)
validation_test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Training (80% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
train_generator = train_datagen.flow_from_directory(
    'F:/CelebAI100/Data_Image_celeb/DataTrain',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical'
)

# ‚úÖ ‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Validation Set (10% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
validation_generator = validation_test_datagen.flow_from_directory(
    'F:/CelebAI100/Data_Image_celeb/DataTest',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',  # ‚úÖ ‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Validation
    shuffle=False
)

# ‚úÖ ‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Test Set (10% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
test_generator = validation_test_datagen.flow_from_directory(
    'F:/CelebAI100/Data_Image_celeb/DataTest',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',  # ‚úÖ ‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Test
    shuffle=False
)

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á CNN Model ‡πÅ‡∏ö‡∏ö Simple (‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ MobileNetV2)
def build_simple_cnn(input_shape, num_classes):
    model = models.Sequential([
        # Block 1
        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        # Block 2 (Depthwise Separable Conv)
        layers.SeparableConv2D(64, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        # Block 3
        layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        # Block 4
        layers.SeparableConv2D(256, (3, 3), padding='same', activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        # Feature Extraction
        layers.GlobalAveragePooling2D(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),  # ‡∏•‡∏î Overfitting

        # Output Layer
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

# ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = build_simple_cnn((img_height, img_width, 3), train_generator.num_classes)

# ‚úÖ Compile Model
optimizer = Adam(learning_rate=initial_learning_rate)
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# ‚úÖ Callbacks
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

model_checkpoint = ModelCheckpoint(
    'best_model_simple_cnn.keras',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# ‚úÖ Train Model
history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=validation_generator,  # ‚úÖ ‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Validation
    callbacks=[early_stopping, model_checkpoint, reduce_lr]
)

# ‚úÖ Save Final Model
model.save('cnn_final_simple_model.keras')
print("Final model saved!")

# ‚úÖ Evaluate Results (‡πÉ‡∏ä‡πâ 50% ‡∏Ç‡∏≠‡∏á DataTest ‡πÄ‡∏õ‡πá‡∏ô Test Set)
y_true = []
y_pred = []
for i in range(len(test_generator)):
    x_test, y_test = test_generator[i]
    predictions = model.predict(x_test)
    y_true.extend(np.argmax(y_test, axis=1))
    y_pred.extend(np.argmax(predictions, axis=1))

# Calculate metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

# Print evaluation metrics
print("\nüìä *Evaluation Metrics on Test Data:*")
print(f"‚úÖ Accuracy: {accuracy:.4f}")
print(f"‚úÖ Precision: {precision:.4f}")
print(f"‚úÖ Recall: {recall:.4f}")
print(f"‚úÖ F1-Score: {f1:.4f}")
